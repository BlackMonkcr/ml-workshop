import requests
from bs4 import BeautifulSoup


def normalize_genre_name(genre_name):
    """
    Los g√©neros en genres.txt ya est√°n normalizados para URLs.
    Esta funci√≥n simplemente retorna el g√©nero tal como est√°.

    Args:
        genre_name (str): Nombre del g√©nero ya normalizado desde genres.txt

    Returns:
        str: Mismo nombre (ya normalizado para URL)
    """
    return genre_name.strip()


def load_genres(path):
    """
    Carga la lista de g√©neros desde un archivo de texto

    Args:
        path (str): Ruta al archivo de g√©neros

    Returns:
        list: Lista de g√©neros
    """
    with open(path, "r", encoding="utf-8") as f:
        genres = [line.strip() for line in f if line.strip()]
    return genres


def save_file(artists, path):
    with open(path, "w") as f:
        for artist in artists:
            f.write(artist + "\n")


def load_file(path):
    with open(path, "r") as f:
        text = f.read()
        text = text.split("\n")

        return text


def read_pickle(filename):
    import pickle

    with open(filename, "rb") as fh:
        list_name = pickle.load(fh)

    return list_name


def save_pickle(filename, obj):
    import pickle

    with open(filename, "wb") as f:
        pickle.dump(obj, f)


def get_artists(base_url, genre):
    """
    Obtiene la lista de artistas m√°s populares de un g√©nero espec√≠fico

    Args:
        base_url (str): URL base de letras.com
        genre (str): G√©nero musical normalizado

    Returns:
        list: Lista de URLs de artistas
    """
    url = f"{base_url}/mais-acessadas/{genre}/"

    try:
        result = requests.get(url)
        result.raise_for_status()  # Lanza excepci√≥n si hay error HTTP
        src = result.content

        soup = BeautifulSoup(src, "lxml")
        links = soup.find("ol", class_="top-list_art")

        if not links:
            print(f"No se encontraron artistas para el g√©nero: {genre}")
            return []

        artists = []
        for link in links.find_all("li"):
            artist_link = link.find("a")
            if artist_link and "href" in artist_link.attrs:
                url_artist = artist_link.attrs["href"]
                artists.append(url_artist)

        print(f"Se encontraron {len(artists)} artistas para el g√©nero: {genre}")
        return artists

    except requests.exceptions.RequestException as e:
        print(f"Error al obtener artistas del g√©nero {genre}: {e}")
        return []
    except Exception as e:
        print(f"Error inesperado al procesar g√©nero {genre}: {e}")
        return []


def extract_songs_from_new_layout(soup):
    """Extrae canciones usando el nuevo layout de letras.com"""
    songs = []
    song_rows = soup.find_all("tr", class_="songList-table-row")

    if song_rows:
        for row in song_rows:
            song_link = row.find("a", href=True)
            if song_link:
                href = song_link.get("href")
                if href and href.startswith("/") and href.count("/") >= 2:
                    songs.append(href)

    return songs


def extract_songs_from_old_layout(soup):
    """Extrae canciones usando el layout antiguo de letras.com"""
    songs = []
    links = soup.find("ul", class_="cnt-list-songs -counter -top-songs js-song-list")

    if links:
        for link in links.find_all("li"):
            song_link = link.find("a")
            if song_link and "href" in song_link.attrs:
                songs.append(song_link.attrs["href"])

    return songs


def extract_songs_fallback(soup, url):
    """M√©todo de fallback para extraer canciones"""
    songs = []
    all_links = soup.find_all("a", href=True)
    artist_name = url.split("/")[-2]

    for link in all_links:
        href = link.get("href", "")
        if (artist_name in href and
            href.startswith("/") and
            href.count("/") >= 2 and
            not any(skip in href for skip in ["ouvir", "radio", "discografia", "biografia"])):
            songs.append(href)

    return songs


def get_song_names(url):
    """
    Obtiene los nombres de las canciones m√°s tocadas de un artista

    Args:
        url (str): URL de la p√°gina de canciones m√°s tocadas del artista

    Returns:
        list: Lista de URLs de canciones
    """
    try:
        print(f"      üîç Accediendo a: {url}")
        result = requests.get(url)
        result.raise_for_status()
        soup = BeautifulSoup(result.content, "lxml")

        # Intentar nuevo layout primero
        songs = extract_songs_from_new_layout(soup)
        if songs:
            print(f"      ‚úÖ Encontradas {len(songs)} canciones (nuevo layout)")
            return songs

        # Intentar layout antiguo
        songs = extract_songs_from_old_layout(soup)
        if songs:
            print(f"      ‚úÖ Encontradas {len(songs)} canciones (layout antiguo)")
            return songs

        # M√©todo de fallback
        print("      üîç Usando m√©todo de fallback...")
        songs = extract_songs_fallback(soup, url)

        # Eliminar duplicados
        unique_songs = list(dict.fromkeys(songs))
        print(f"      ‚úÖ Encontradas {len(unique_songs)} canciones √∫nicas")
        return unique_songs

    except requests.exceptions.RequestException as e:
        print(f"      ‚ùå Error de conexi√≥n en {url}: {e}")
        return []
    except Exception as e:
        print(f"      ‚ùå Error inesperado en {url}: {e}")
        return []


def extract_lyrics_new_layout(soup):
    """Extrae letras usando el nuevo layout de letras.com"""
    lyrics_text = ""
    composer_info = ""

    # Buscar contenedores modernos de letras
    possible_selectors = [
        {"tag": "div", "class": lambda x: x and "lyric" in str(x).lower()},
        {"tag": "div", "class": lambda x: x and "letra" in str(x).lower()},
        {"tag": "section", "class": lambda x: x and ("lyric" in str(x).lower() or "letra" in str(x).lower())},
    ]

    for selector in possible_selectors:
        container = soup.find(selector["tag"], class_=selector["class"])
        if container:
            # Extraer texto de p√°rrafos
            paragraphs = container.find_all("p")
            if paragraphs:
                lyrics_parts = []
                for p in paragraphs:
                    text = str(p)
                    text = text.replace("<p>", "").replace("</p>", "")
                    text = text.replace("<br/>", "\n").replace("<br>", "\n")
                    lyrics_parts.append(text)
                lyrics_text = "\n\n".join(lyrics_parts)
                break
            else:
                # Si no hay p√°rrafos, usar el texto completo
                lyrics_text = container.get_text(separator="\n")
                break

    # Buscar informaci√≥n del compositor con selectores modernos
    comp_selectors = [
        "div[class*='composer']",
        "div[class*='author']",
        "div[class*='comp']",
        "span[class*='composer']",
        "span[class*='author']"
    ]

    for selector in comp_selectors:
        comp_elem = soup.select_one(selector)
        if comp_elem:
            composer_info = comp_elem.get_text(strip=True)
            break

    return lyrics_text, composer_info


def extract_lyrics_old_layout(soup):
    """Extrae letras usando el layout antiguo de letras.com"""
    lyrics_text = ""
    composer_info = ""

    # Selector original
    container = soup.find("div", class_="cnt-letra p402_premium")
    if container:
        paragraphs = container.find_all("p")
        if paragraphs:
            lyrics_parts = []
            for p in paragraphs:
                text = str(p)
                text = text.replace("<p>", "").replace("</p>", "")
                text = text.replace("<br/>", "\n").replace("<br>", "\n")
                lyrics_parts.append(text)
            lyrics_text = "\n\n".join(lyrics_parts)

    # Compositor original
    comp_elem = soup.find("div", class_="letra-info_comp")
    if comp_elem:
        composer_info = comp_elem.get_text(strip=True)

    return lyrics_text, composer_info


def extract_lyrics_fallback(soup):
    """M√©todo de fallback para extraer letras"""
    lyrics_text = ""
    composer_info = ""

    # Buscar el elemento con m√°s texto (probablemente las letras)
    text_elements = []
    for elem in soup.find_all(['div', 'p', 'section']):
        text = elem.get_text(strip=True)
        if len(text) > 100:  # Elementos con bastante texto
            text_elements.append((elem, len(text)))

    if text_elements:
        # Ordenar por longitud de texto y tomar el m√°s largo
        text_elements.sort(key=lambda x: x[1], reverse=True)
        longest_elem = text_elements[0][0]
        lyrics_text = longest_elem.get_text(separator="\n", strip=True)

    return lyrics_text, composer_info


def get_lyrics(url):
    """
    Extrae las letras de una canci√≥n desde su URL

    Args:
        url (str): URL de la p√°gina de la canci√≥n

    Returns:
        tuple: (letras, compositor)
    """
    try:
        result = requests.get(url)
        result.raise_for_status()
        soup = BeautifulSoup(result.content, "lxml")

        # Intentar nuevo layout primero
        lyrics, composer = extract_lyrics_new_layout(soup)
        if lyrics:
            return lyrics, composer

        # Intentar layout antiguo
        lyrics, composer = extract_lyrics_old_layout(soup)
        if lyrics:
            return lyrics, composer

        # M√©todo de fallback
        lyrics, composer = extract_lyrics_fallback(soup)
        if lyrics:
            return lyrics, composer

        # Si nada funciona
        return "", ""

    except Exception as e:
        print(f"Error procesando {url}: {e}")
        song = url.split("/")[-1] if url.split("/")[-1] else url.split("/")[-2]
        print(f"Error on song {song}")
        return "", ""


if __name__ == "__main__":
    import os
    import time

    # Configuraci√≥n general
    base_url = "https://www.letras.com"
    end_url = "mais-tocadas.html"
    genres_path = "genres.txt"
    artists_path = "artists_by_genre.pickle"
    lyrics_path = "lyrics_by_genre.pickle"

    # L√≠mites de procesamiento
    MAX_ARTISTS_PER_GENRE = 50  # Reducido para manejar m√∫ltiples g√©neros
    MAX_SONGS_PER_ARTIST = 10   # Limitar canciones por artista

    print("üéµ Iniciando extracci√≥n de letras multi-g√©nero üéµ")
    print("=" * 50)

    # Cargar g√©neros
    if not os.path.exists(genres_path):
        print(f"‚ùå Error: No se encontr√≥ el archivo {genres_path}")
        exit(1)

    genres = load_genres(genres_path)
    print(f"üìÇ Se cargaron {len(genres)} g√©neros musicales")

    # Cargar o inicializar estructura de artistas por g√©nero
    if os.path.exists(artists_path):
        artists_by_genre = read_pickle(artists_path)
        print("üìÅ Cargada lista existente de artistas por g√©nero")
    else:
        artists_by_genre = {}
        print("üÜï Creando nueva estructura de artistas por g√©nero")

    # Cargar o inicializar estructura de letras por g√©nero
    if os.path.exists(lyrics_path):
        lyrics_by_genre = read_pickle(lyrics_path)
        print("üìÅ Cargada base de datos existente de letras")
    else:
        lyrics_by_genre = {}
        print("üÜï Creando nueva base de datos de letras")

    # Procesar cada g√©nero
    for i, genre in enumerate(genres, 1):
        print(f"\nüé∏ Procesando g√©nero {i}/{len(genres)}: {genre}")
        print("-" * 40)

        # Los g√©neros ya est√°n normalizados en genres.txt
        normalized_genre = normalize_genre_name(genre)
        print(f"üîó Procesando: {normalized_genre}")

        # Verificar si ya tenemos artistas para este g√©nero
        if genre not in artists_by_genre:
            print(f"üîç Buscando artistas de {genre}...")
            artists = get_artists(base_url, normalized_genre)
            if artists:
                artists_by_genre[genre] = artists
                save_pickle(artists_path, artists_by_genre)
                print(f"‚úÖ Guardados {len(artists)} artistas de {genre}")
            else:
                print(f"‚ö†Ô∏è  No se encontraron artistas para {genre}")
                continue
        else:
            artists = artists_by_genre[genre]
            print(f"üìã Ya tenemos {len(artists)} artistas de {genre}")

        # Inicializar estructura de letras para este g√©nero si no existe
        if genre not in lyrics_by_genre:
            lyrics_by_genre[genre] = {}

        # Procesar artistas del g√©nero (limitado)
        artists_to_process = artists[:MAX_ARTISTS_PER_GENRE]
        print(f"üë• Procesando {len(artists_to_process)} artistas de {genre}")

        for j, artist in enumerate(artists_to_process, 1):
            print(f"  üé§ Artista {j}/{len(artists_to_process)}: {artist}")

            # Verificar si ya procesamos este artista
            if artist in lyrics_by_genre[genre]:
                print(f"    ‚úì Ya procesado: {artist}")
                continue

            try:
                # Obtener canciones del artista
                url_song = base_url + artist + end_url
                songs = get_song_names(url_song)

                if not songs:
                    print(f"    ‚ö†Ô∏è  No se encontraron canciones para {artist}")
                    continue

                # Limitar n√∫mero de canciones
                songs_to_process = songs[:MAX_SONGS_PER_ARTIST]
                lyrics_by_genre[genre][artist] = {}

                # Procesar canciones
                for k, song in enumerate(songs_to_process, 1):
                    song_name = song.split('/')[-2] if song.endswith('/') else song.split('/')[-1]
                    print(f"    üéµ Canci√≥n {k}/{len(songs_to_process)}: {song_name}")

                    url_lyrics = base_url + song
                    text, by = get_lyrics(url_lyrics)

                    if text:
                        print(f"      ‚úÖ Letras extra√≠das ({len(text)} caracteres)")
                    else:
                        print("      ‚ö†Ô∏è  No se pudieron extraer letras")

                    # Guardar letra
                    lyrics_by_genre[genre][artist][song] = {
                        'lyrics': text,
                        'composer': by,
                        'genre': genre
                    }

                    time.sleep(1)  # Pausa entre canciones

                print(f"    ‚úÖ Procesadas {len(songs_to_process)} canciones de {artist}")

                # Guardar progreso despu√©s de cada artista
                save_pickle(lyrics_path, lyrics_by_genre)
                time.sleep(2)  # Pausa entre artistas

            except Exception as e:
                print(f"    ‚ùå Error procesando {artist}: {e}")
                continue

        print(f"‚úÖ Completado g√©nero: {genre}")

        # Pausa entre g√©neros
        if i < len(genres):
            print("‚è≥ Pausa entre g√©neros...")
            time.sleep(5)

    print("\nüéâ ¬°Extracci√≥n completada!")
    print("=" * 50)

    # Mostrar estad√≠sticas finales
    total_genres = len(lyrics_by_genre)
    total_artists = sum(len(artists) for artists in lyrics_by_genre.values())
    total_songs = sum(
        len(songs)
        for genre_data in lyrics_by_genre.values()
        for songs in genre_data.values()
    )

    print("üìä Estad√≠sticas finales:")
    print(f"   üé∏ G√©neros procesados: {total_genres}")
    print(f"   üë• Artistas procesados: {total_artists}")
    print(f"   üéµ Canciones extra√≠das: {total_songs}")
    print(f"   üíæ Datos guardados en: {lyrics_path}")
